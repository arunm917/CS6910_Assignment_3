{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arunm917/CS6910_Assignment_3/blob/main/CS6910_Assignment_3_PartB_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uixetIxwm_qE"
      },
      "source": [
        "# Downloading necessary packages and files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gkx75o8qZtGB"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import gdown\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Jkkc0LcC4whY"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHm92AkybYZb",
        "outputId": "bfdc861c-1d1e-4045-b804-e99232bfdf89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1pdJVD8P71fpqGRnvFfOp_6TbVft9NlnH\n",
            "To: /content/tam_train\n",
            "100%|██████████| 2.69M/2.69M [00:00<00:00, 123MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# downloading file from gdrive\n",
        "output = 'tam_train'\n",
        "file_id = '1pdJVD8P71fpqGRnvFfOp_6TbVft9NlnH' # Google drive ID\n",
        "#Download the file\n",
        "gdown.download('https://drive.google.com/uc?id=' + file_id, output, quiet=False)\n",
        "print('DONE.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AVeCbeNnQuw",
        "outputId": "e92d9e21-678f-449a-95af-cf904c41796f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1pdp6ojHltRRNLXsmoQbGRc2Qn8X1EUJV\n",
            "To: /content/tam_valid\n",
            "100%|██████████| 164k/164k [00:00<00:00, 79.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# downloading file from gdrive\n",
        "output = 'tam_valid'\n",
        "file_id = '1pdp6ojHltRRNLXsmoQbGRc2Qn8X1EUJV' # Google drive ID\n",
        "#Download the file\n",
        "gdown.download('https://drive.google.com/uc?id=' + file_id, output, quiet=False)\n",
        "print('DONE.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgydhFEdnUZI",
        "outputId": "87880660-19a8-4d8b-e54f-1085fd139b92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1pdaTq-g2ZKhRKv6fRrSbEsJkOH5gdrEQ\n",
            "To: /content/tam_test\n",
            "100%|██████████| 157k/157k [00:00<00:00, 78.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# downloading file from gdrive\n",
        "output = 'tam_test'\n",
        "file_id = '1pdaTq-g2ZKhRKv6fRrSbEsJkOH5gdrEQ' # Google drive ID\n",
        "#Download the file\n",
        "gdown.download('https://drive.google.com/uc?id=' + file_id, output, quiet=False)\n",
        "print('DONE.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEItPR0Hp904"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yKNN0xMhaI4L"
      },
      "outputs": [],
      "source": [
        "train_data_df = pd.read_csv('tam_train')\n",
        "valid_data_df = pd.read_csv('tam_valid')\n",
        "test_data_df = pd.read_csv('tam_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wiKR6-yZ4pKX"
      },
      "outputs": [],
      "source": [
        "train_data_df.columns = ['English','Tamil']\n",
        "valid_data_df.columns = ['English','Tamil']\n",
        "test_data_df.columns = ['English','Tamil']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6B8VQgLq0Na"
      },
      "source": [
        "# Creating vocabulary and padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oRuAnljKTPk9"
      },
      "outputs": [],
      "source": [
        "# Checkign unique chars\n",
        "############################################## Train data #########################################################\n",
        "char_list_eng_train = []\n",
        "for i in range(len(train_data_df['English'])):\n",
        "  char = [*train_data_df.loc[i, 'English']]\n",
        "  char_list_eng_train.extend(char)\n",
        "\n",
        "char_list_tam_train = []\n",
        "for i in range(len(train_data_df['Tamil'])):\n",
        "  char = [*train_data_df.loc[i, 'Tamil']]\n",
        "  char_list_tam_train.extend(char)\n",
        "\n",
        "############################################## Validation data #########################################################\n",
        "char_list_eng_val = []\n",
        "for i in range(len(valid_data_df['English'])):\n",
        "  char = [*valid_data_df.loc[i, 'English']]\n",
        "  char_list_eng_val.extend(char)\n",
        "\n",
        "char_list_tam_val = []\n",
        "for i in range(len(valid_data_df['Tamil'])):\n",
        "  char = [*valid_data_df.loc[i, 'Tamil']]\n",
        "  char_list_tam_val.extend(char)\n",
        "\n",
        "############################################## Test data #########################################################\n",
        "char_list_eng_test = []\n",
        "for i in range(len(test_data_df['English'])):\n",
        "  char = [*test_data_df.loc[i, 'English']]\n",
        "  char_list_eng_test.extend(char)\n",
        "\n",
        "char_list_tam_test = []\n",
        "for i in range(len(test_data_df['Tamil'])):\n",
        "  char = [*test_data_df.loc[i, 'Tamil']]\n",
        "  char_list_tam_test.extend(char)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tam_char_train = list(set(char_list_tam_train))\n",
        "unique_tam_char_val = list(set(char_list_tam_val))\n",
        "unique_tam_char_test = list(set(char_list_tam_test))"
      ],
      "metadata": {
        "id": "qTykC9ia1k8a"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0; j=0\n",
        "for char in unique_tam_char_train:\n",
        "  # print(char)\n",
        "  if char in unique_tam_char_val:\n",
        "    j += 1\n",
        "  else:\n",
        "    i = i + 1\n",
        "print('No. of char in test but not in train:', i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMRcwwWp1g_z",
        "outputId": "e3d5b65f-e496-4197-a7bf-9717fa0f2c07"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of char in test but not in train: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for char in unique_tam_char_test:\n",
        "  if char in unique_tam_char_train:\n",
        "    i = 0\n",
        "  else:\n",
        "    i += 1\n",
        "print('No. of char in test but not in train:', i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpn-xRpZ2uwS",
        "outputId": "1b02a511-53ba-4bae-b3e9-37fc0b13c44d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of char in test but not in train: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UB3ZXOTcZD5s"
      },
      "outputs": [],
      "source": [
        "# Indexing\n",
        "SOS_token = '<SOS>'\n",
        "EOS_token = '<EOS>'\n",
        "PAD_token = '<PAD>'\n",
        "UNK_token = '<UNK>'\n",
        "\n",
        "vocabulary_eng = list(set(char_list_eng_train))\n",
        "vocabulary_eng = [PAD_token] + [UNK_token] + [SOS_token] + [EOS_token] + vocabulary_eng \n",
        "\n",
        "vocabulary_tam = list(set(char_list_tam_train))\n",
        "vocabulary_tam = [PAD_token] + [UNK_token] + [SOS_token] + [EOS_token] + vocabulary_tam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "O8vdEmMsbw6U"
      },
      "outputs": [],
      "source": [
        "char_index_eng = {value: index for index, value in enumerate(vocabulary_eng)}\n",
        "char_index_tam = {value: index for index, value in enumerate(vocabulary_tam)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "b4N10f0eMJ7N"
      },
      "outputs": [],
      "source": [
        "idx2char_eng = {value: key for key, value in char_index_eng.items()}\n",
        "idx2char_tam = {value: key for key, value in char_index_tam.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DWrNfanAtOvi"
      },
      "outputs": [],
      "source": [
        "# Define the tokenizer\n",
        "\n",
        "def tokenize_eng(word):\n",
        "    chars = [*word]\n",
        "    tokens_eng = [char_index_eng[char] if char in char_index_eng else 0 for char in chars]\n",
        "    \n",
        "    return tokens_eng\n",
        "\n",
        "def tokenize_tam(word):\n",
        "    chars = [*word]\n",
        "    tokens_tam = [char_index_tam[char] if char in char_index_tam else 0 for char in chars]\n",
        "    \n",
        "    return tokens_tam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mDoAvRMB050I"
      },
      "outputs": [],
      "source": [
        "# Define the training pairs\n",
        "training_pairs = train_data_df.values.tolist()\n",
        "val_pairs = valid_data_df.values.tolist()\n",
        "test_pairs = test_data_df.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BnKBJPS8BVdM"
      },
      "outputs": [],
      "source": [
        "eng_words = [tokenize_eng(pair[0]) for pair in training_pairs]\n",
        "tam_words = [tokenize_tam(pair[1]) for pair in training_pairs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jFD9A6kACNa9"
      },
      "outputs": [],
      "source": [
        "# Determining max length english\n",
        "\n",
        "lengths_eng = []\n",
        "\n",
        "for word in eng_words:\n",
        "\n",
        "    word_length = len(word)\n",
        "    lengths_eng.append(word_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bXirSnvwMRc0"
      },
      "outputs": [],
      "source": [
        "# Determining max length tamil\n",
        "max_length_tam = max([len(words) for words in tam_words])\n",
        "\n",
        "# Determining max length english and tamil\n",
        "max_length = max([len(words) for words in eng_words + tam_words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6jJPAiIbL_cE"
      },
      "outputs": [],
      "source": [
        "def padding(word_pairs):\n",
        "  ''' Function to pad the input and target sequences. Padding is done to ensure that\n",
        "      all the training, validation and test samples are of equal size.'''\n",
        "  \n",
        "  eng_words = [tokenize_eng(pair[0]) for pair in word_pairs]\n",
        "  tam_words = [tokenize_tam(pair[1]) for pair in word_pairs]\n",
        "\n",
        "  \n",
        "  padded_input_sequences = [torch.tensor([char_index_eng['<SOS>']] + eng_words + [char_index_eng['<EOS>']] + [(char_index_eng['<PAD>'])]*(max_length - len(eng_words))) for eng_words in eng_words]\n",
        "  padded_target_sequences = [torch.tensor([char_index_eng['<SOS>']] + tam_words + [char_index_tam['<EOS>']] + [(char_index_tam['<PAD>'])]*(max_length - len(tam_words))) for tam_words in tam_words]\n",
        "  tensor = torch.tensor([char_index_eng['<PAD>']]*(max_length+2))\n",
        "  padded_input_sequences.append(tensor)\n",
        "  padded_target_sequences.append(tensor)\n",
        "  padded_input_sequences = torch.stack(padded_input_sequences)\n",
        "  padded_target_sequences = torch.stack(padded_target_sequences)\n",
        "  \n",
        "  return(padded_input_sequences,padded_target_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1mLjEjNevW_4"
      },
      "outputs": [],
      "source": [
        "# Creating datasets\n",
        "training_input_sequences, training_target_sequences = padding(training_pairs)\n",
        "train_dataset = torch.utils.data.TensorDataset(training_input_sequences, training_target_sequences)\n",
        "\n",
        "val_input_sequences, val_target_sequences = padding(val_pairs)\n",
        "val_dataset = torch.utils.data.TensorDataset(val_input_sequences, val_target_sequences)\n",
        "\n",
        "test_input_sequences, test_target_sequences = padding(test_pairs)\n",
        "test_dataset = torch.utils.data.TensorDataset(test_input_sequences, test_target_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNOalLjTgryp"
      },
      "source": [
        "# Architecture"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, bidirectionality, cell_type_encoder):\n",
        "        super(Encoder, self).__init__()\n",
        "        ''' The encoder encodes the input characters and converts it into a hidden representation'''\n",
        "        self.bidirectionality = bidirectionality\n",
        "\n",
        "        if self.bidirectionality == 'YES':\n",
        "          bidirectional = True\n",
        "          self.directions = 2\n",
        "        else:\n",
        "          bidirectional = False\n",
        "          self.directions = 1\n",
        "\n",
        "        self.cell_type_encoder = cell_type_encoder\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "\n",
        "        if self.cell_type_encoder == 'RNN':\n",
        "          self.rnn = nn.RNN(embedding_size, hidden_size, num_layers,bidirectional = bidirectional, dropout=p)\n",
        "        if self.cell_type_encoder == 'LSTM':\n",
        "          self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers,bidirectional = bidirectional, dropout=p)\n",
        "        if self.cell_type_encoder == 'GRU':\n",
        "          self.rnn = nn.GRU(embedding_size, hidden_size, num_layers,bidirectional = bidirectional, dropout=p)\n",
        "\n",
        "        self.fc_hidden = nn.Linear(hidden_size*self.directions, hidden_size)\n",
        "        self.fc_cell = nn.Linear(hidden_size*self.directions, hidden_size)\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "        if self.cell_type_encoder == 'LSTM':\n",
        "          encoder_outputs, (hidden, cell) = self.rnn(embedding)\n",
        "        else:\n",
        "          encoder_outputs, hidden = self.rnn(embedding)\n",
        "\n",
        "############### Applying Bidirectionality ###########################\n",
        "\n",
        "        if self.cell_type_encoder == 'LSTM':\n",
        "          if self.bidirectionality == 'YES':\n",
        "            row = 1\n",
        "            hidden_list = []\n",
        "            cell_list = []\n",
        "            for i in range(hidden.shape[0]//2):\n",
        "              hidden_concatenated = self.fc_hidden(torch.cat((hidden[row-1:row], hidden[row:row+1]), dim=2))\n",
        "              cell_concatenated = self.fc_cell(torch.cat((cell[row-1:row], cell[row:row+1]), dim=2))\n",
        "              hidden_list.append(hidden_concatenated)\n",
        "              cell_list.append(cell_concatenated)\n",
        "              row += 2\n",
        "\n",
        "            hidden_tensor = torch.stack(hidden_list)\n",
        "            cell_tensor = torch.stack(cell_list)\n",
        "            hidden_squeezed = hidden_tensor.squeeze()\n",
        "            cell_squeezed = cell_tensor.squeeze()\n",
        "          else:\n",
        "            hidden_squeezed = hidden\n",
        "            cell_squeezed = cell\n",
        "        else:\n",
        "          if self.bidirectionality == 'YES':\n",
        "            row = 1\n",
        "            hidden_list = []\n",
        "            for i in range(hidden.shape[0]//2):\n",
        "              hidden_concatenated = self.fc_hidden(torch.cat((hidden[row-1:row], hidden[row:row+1]), dim=2))\n",
        "              hidden_list.append(hidden_concatenated)\n",
        "              row += 2\n",
        "\n",
        "            hidden_tensor = torch.stack(hidden_list)\n",
        "            hidden_squeezed = hidden_tensor.squeeze()\n",
        "          else:\n",
        "            hidden_squeezed = hidden\n",
        "\n",
        "        if self.cell_type_encoder == 'LSTM':\n",
        "          return encoder_outputs, hidden_squeezed, cell_squeezed\n",
        "        else:\n",
        "          return encoder_outputs, hidden_squeezed\n"
      ],
      "metadata": {
        "id": "sE5HL5KeBOkS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p, cell_type_decoder, bidirectionality):\n",
        "        super(Decoder, self).__init__()\n",
        "        ''' The decoder decodes the input from the encoder and produces an output at each timestep '''\n",
        "        self.bidirectionality = bidirectionality\n",
        "        if self.bidirectionality == 'YES':\n",
        "          bidirectional = True\n",
        "          self.directions = 2\n",
        "        else:\n",
        "          bidirectional = False\n",
        "          self.directions = 1\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type_decoder = cell_type_decoder\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        if self.cell_type_decoder == 'RNN':\n",
        "          self.rnn = nn.RNN((hidden_size*self.directions + embedding_size), hidden_size, num_layers, dropout=p)\n",
        "        if self.cell_type_decoder == 'LSTM':\n",
        "          self.rnn = nn.LSTM((hidden_size*self.directions + embedding_size), hidden_size, num_layers, dropout=p)\n",
        "        if self.cell_type_decoder == 'GRU':\n",
        "          self.rnn = nn.GRU((hidden_size*self.directions + embedding_size), hidden_size, num_layers, dropout=p)\n",
        "        \n",
        "        self.energy = nn.Linear(hidden_size*(self.directions + 1), 1)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.softmax = nn.Softmax(dim = 0)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, encoder_outputs, hidden, cell = None):\n",
        "        x = x.unsqueeze(0)\n",
        "\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "        input_length = encoder_outputs.shape[0] # Decoder input is encoder output\n",
        "\n",
        "        encoder_outputs_reshaped = encoder_outputs.repeat(num_layers,1,1)\n",
        "        hidden_reshaped = hidden.repeat(input_length, 1, 1 ) # Reshaping decoder hidden so that it can be concatenated\n",
        "\n",
        "        energy = self.relu(self.energy(torch.cat((hidden_reshaped, encoder_outputs_reshaped), dim = 2)))\n",
        "        self.attention_scores = self.softmax(energy)\n",
        "        ## Using einsum to get the respective element wise products\n",
        "        context_vector = torch.einsum(\"snk,snl->knl\", self.attention_scores, encoder_outputs_reshaped)\n",
        "        rnn_input = torch.cat((context_vector, embedding), dim = 2)\n",
        "\n",
        "        if self.cell_type_decoder == 'LSTM':\n",
        "          outputs, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
        "        else:\n",
        "          outputs, hidden = self.rnn(rnn_input, hidden)\n",
        "\n",
        "        predictions = self.fc(outputs)\n",
        "        predictions = predictions.squeeze(0)\n",
        "\n",
        "        if self.cell_type_decoder == 'LSTM':\n",
        "          return predictions, hidden, cell, self.attention_scores\n",
        "        else:\n",
        "          return predictions, hidden, self.attention_scores"
      ],
      "metadata": {
        "id": "yYYKaTSEBWll"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, cell_type_encoder, cell_type_decoder, bidirectionality):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        ''' This is the model. This function recieves the input and sends it to the encoder and receives the output \n",
        "            and the hidden states from the encoder and sends it to the decoder for decoding'''\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.cell_type_encoder = cell_type_encoder\n",
        "        self.cell_type_decoder = cell_type_decoder\n",
        "        if bidirectionality == 'YES':\n",
        "          bidirectional = True\n",
        "          self.directions = 2\n",
        "        else:\n",
        "          bidirectional = False\n",
        "          self.directions = 1\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "        batch_size = source.shape[1]\n",
        "        target_len = target.shape[0]\n",
        "        target_vocab_size = len(vocabulary_tam)\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "        if self.cell_type_encoder != 'LSTM':\n",
        "          cell = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
        "\n",
        "        if self.cell_type_encoder == 'LSTM':\n",
        "          encoder_outputs, hidden, cell = self.encoder(source)\n",
        "        else:\n",
        "          encoder_outputs, hidden = self.encoder(source)\n",
        "\n",
        "        # first input to the Decoder will be <SOS> token\n",
        "        x = target[0]\n",
        "        predicted_sequences = torch.zeros([32, batch_size]).to(device)\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            if self.cell_type_decoder == 'LSTM':\n",
        "              output, hidden, cell, attention_scores_batch = self.decoder(x, encoder_outputs, hidden, cell)\n",
        "            else:\n",
        "              output, hidden, attention_scores_batch = self.decoder(x, encoder_outputs, hidden)\n",
        "\n",
        "            # Store next output prediction\n",
        "            outputs[t] = output\n",
        "\n",
        "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
        "            predicted_token = output.argmax(1)\n",
        "            x = target[t] if random.random() < teacher_force_ratio else predicted_token\n",
        "\n",
        "            predicted_sequences[t] = predicted_token.squeeze()\n",
        "        \n",
        "        predicted_sequences_copy = predicted_sequences[1:].t()\n",
        "        target_copy = target[1:].t()\n",
        "        correct_predictions_batch = correct_sequences_count(predicted_sequences_copy, target_copy)\n",
        "        return outputs, correct_predictions_batch, attention_scores_batch"
      ],
      "metadata": {
        "id": "HmX2n-uzBlIM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' This function is used to aggregate the predicted vectors and check if the predictions match the\n",
        "    target words. The function returns the number of correct predictions per batch'''\n",
        "def correct_sequences_count(predicted_sequences, target_sequences):\n",
        "  \n",
        "    correct_predictions_batch = 0\n",
        "    words = []\n",
        "    for i in range(batch_size):\n",
        "        # print('predicted sequence:', predicted_sequences[i].shape)\n",
        "        # print('target sequence:', target_tensor[i].shape)\n",
        "        target_word_list = []\n",
        "        target_word_length = 0\n",
        "        predicted_word_length = 0\n",
        "        flag_target = 1\n",
        "        flag_predicted = 1\n",
        "\n",
        "        for element in target_sequences[i]:\n",
        "            idx = element.item()\n",
        "            target_char =  idx2char_tam[idx]\n",
        "            target_word_list.append(target_char)\n",
        "            if flag_target == 1:\n",
        "              target_word_length += 1\n",
        "              if idx == char_index_tam['<EOS>']:\n",
        "                flag_target = 0\n",
        "                break\n",
        "\n",
        "        target_word_length = target_word_length - 1\n",
        "        target_word = ''.join(target_word_list[:-1])\n",
        "    \n",
        "        predicted_word_list = []\n",
        "        for element in predicted_sequences[i]:\n",
        "            idx = element.item()\n",
        "            predicted_char =  idx2char_tam[idx]\n",
        "            predicted_word_list.append(predicted_char)\n",
        "            if flag_predicted == 1:\n",
        "              predicted_word_length += 1\n",
        "              if idx == char_index_tam['<EOS>']:\n",
        "                flag_predicted = 0\n",
        "                break\n",
        "        \n",
        "        predicted_word_length = predicted_word_length - 1\n",
        "        predicted_word = ''.join(predicted_word_list[:-1])\n",
        "        words.append([target_word, predicted_word])\n",
        "        \n",
        "        if target_word_length == predicted_word_length:\n",
        "          if all(x == y for x, y in zip(target_word_list, predicted_word_list)):\n",
        "              correct_predictions_batch += 1\n",
        "              \n",
        "    '''Use the below lines only when the predictions need to be written to a text file'''\n",
        "    # with open('predictions_attention.txt', 'w') as file:\n",
        "    #     # Write each predicted word to the file\n",
        "    #     for word in words[:-1]:\n",
        "    #         line = str(word) + '\\n'\n",
        "    #         file.write(line)\n",
        "    ''' End of code for writing into text file '''\n",
        "    return correct_predictions_batch"
      ],
      "metadata": {
        "id": "ARtny7rgRiGf"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' This function predicts the accuracy by getting the correct words from correct_sequences_count function'''\n",
        "def accuracy(dataloader):\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    total_loss = 0\n",
        "    correct_predictions_total = 0\n",
        "    correct_predictions_batch = 0\n",
        "\n",
        "    for batch_idx, (input_seq, target_seq) in enumerate(dataloader):\n",
        "        batch_idx += 1\n",
        "        # Get input and targets and get to cuda\n",
        "        inp_data = input_seq.t().to(device)\n",
        "        target = target_seq.t().to(device)\n",
        "\n",
        "        # Forward prop\n",
        "        output, correct_predictions_batch, attention_scores_batch = model(inp_data, target)\n",
        "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
        "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
        "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
        "        # way that we have output_words * batch_size that we want to send in into\n",
        "        # our cost function, so we need to do some reshapin. While we're at it\n",
        "        # Let's also remove the start token while we're at it\n",
        "        \n",
        "        output = output[1:].reshape(-1, output.shape[2])\n",
        "        target = target[1:].reshape(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, target)\n",
        "        total_loss += loss.item()\n",
        "        correct_predictions_total += correct_predictions_batch\n",
        "    eval_loss = total_loss/batch_idx\n",
        "    accuracy = (correct_predictions_total/((batch_idx*batch_size) - 1))*100\n",
        "    model.train()\n",
        "  return eval_loss, accuracy, attention_scores_batch"
      ],
      "metadata": {
        "id": "M26OZJMzZYrN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP7BX91WRUeS"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "VshMHcHWyoa5"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "num_epochs = 16\n",
        "learning_rate = 0.001\n",
        "batch_size = 256\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_size_encoder = len(vocabulary_eng)\n",
        "input_size_decoder = len(vocabulary_tam)\n",
        "output_size = len(vocabulary_tam)\n",
        "encoder_embedding_size = 200\n",
        "decoder_embedding_size = 200\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "enc_dropout = 0.5\n",
        "dec_dropout = 0.5\n",
        "bidirectionality = 'NO'\n",
        "cell_type_encoder = 'GRU'\n",
        "cell_type_decoder = 'LSTM'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout, bidirectionality, cell_type_encoder).to(device)\n",
        "decoder_net = Decoder(input_size_decoder,decoder_embedding_size, hidden_size, output_size, num_layers, dec_dropout, cell_type_decoder, bidirectionality).to(device)"
      ],
      "metadata": {
        "id": "bkv626hTF5L7"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Seq2Seq(encoder_net, decoder_net, cell_type_encoder, cell_type_decoder, bidirectionality).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "pad_idx = char_index_eng['<PAD>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "scheduler = StepLR(optimizer, step_size = 5, gamma = 0.5)"
      ],
      "metadata": {
        "id": "79LI_XBFG8ey"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4In6N1XzgpJd"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "_JaFVJCnIY3V"
      },
      "outputs": [],
      "source": [
        "# Creating Dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True, num_workers = 2)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size, shuffle=True, num_workers = 2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "\n",
        "    total_loss = 0\n",
        "    correct_predictions_epoch = 0\n",
        "    correct_predictions_batch = 0\n",
        "\n",
        "    for batch_idx, (input_seq, target_seq) in enumerate(train_loader):\n",
        "        batch_idx += 1\n",
        "        # Get input and targets and get to cuda\n",
        "        inp_data = input_seq.t().to(device)\n",
        "        target = target_seq.t().to(device)\n",
        "\n",
        "        # Forward prop\n",
        "        output, correct_predictions_batch, attention_scores_batch = model(inp_data, target)\n",
        "        output = output[1:].reshape(-1, output.shape[2])\n",
        "        target = target[1:].reshape(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, target)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Back prop\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip to avoid exploding gradient issues\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "        # Gradient descent step\n",
        "        optimizer.step()\n",
        "        correct_predictions_epoch += correct_predictions_batch\n",
        "    scheduler.step()\n",
        "    loss_epoch = total_loss/batch_idx\n",
        "    train_accuracy = (correct_predictions_epoch/((batch_idx*batch_size) - 1))*100\n",
        "    val_loss, val_accuracy, attention_scores_batch = accuracy(val_loader)\n",
        "    print('\\nEpoc loss: %.4f' % loss_epoch, '\\nCorrect predictions during epoch:',correct_predictions_epoch,\n",
        "          '\\nTraining accuracy: %.2f'% train_accuracy)\n",
        "    print('\\nValidation loss: %.4f'% val_loss, '\\nValidation accuracy: %.2f'% val_accuracy)\n"
      ],
      "metadata": {
        "id": "QNT0YIUwdybM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b37387fe-4760-4d6f-e351-f25f6e227412"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▋         | 1/16 [02:22<35:31, 142.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 2.3099 \n",
            "Correct predictions during epoch: 7 \n",
            "Training accuracy: 0.01\n",
            "\n",
            "Validation loss: 1.4719 \n",
            "Validation accuracy: 0.76\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▎        | 2/16 [04:47<33:37, 144.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.9104 \n",
            "Correct predictions during epoch: 4978 \n",
            "Training accuracy: 9.72\n",
            "\n",
            "Validation loss: 0.6109 \n",
            "Validation accuracy: 37.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▉        | 3/16 [07:12<31:18, 144.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.3888 \n",
            "Correct predictions during epoch: 20846 \n",
            "Training accuracy: 40.72\n",
            "\n",
            "Validation loss: 0.4980 \n",
            "Validation accuracy: 53.63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 4/16 [09:37<28:57, 144.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.2583 \n",
            "Correct predictions during epoch: 28137 \n",
            "Training accuracy: 54.96\n",
            "\n",
            "Validation loss: 0.4647 \n",
            "Validation accuracy: 56.63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███▏      | 5/16 [12:03<26:35, 145.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.2045 \n",
            "Correct predictions during epoch: 31547 \n",
            "Training accuracy: 61.62\n",
            "\n",
            "Validation loss: 0.4620 \n",
            "Validation accuracy: 58.24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 6/16 [14:28<24:11, 145.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.1421 \n",
            "Correct predictions during epoch: 36101 \n",
            "Training accuracy: 70.51\n",
            "\n",
            "Validation loss: 0.4726 \n",
            "Validation accuracy: 60.44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 7/16 [16:54<21:47, 145.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.1176 \n",
            "Correct predictions during epoch: 37975 \n",
            "Training accuracy: 74.17\n",
            "\n",
            "Validation loss: 0.4502 \n",
            "Validation accuracy: 61.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 8/16 [19:19<19:22, 145.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.1026 \n",
            "Correct predictions during epoch: 39053 \n",
            "Training accuracy: 76.28\n",
            "\n",
            "Validation loss: 0.4853 \n",
            "Validation accuracy: 61.15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▋    | 9/16 [21:44<16:56, 145.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.0937 \n",
            "Correct predictions during epoch: 40076 \n",
            "Training accuracy: 78.27\n",
            "\n",
            "Validation loss: 0.4835 \n",
            "Validation accuracy: 61.61\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▎   | 10/16 [24:09<14:31, 145.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.0869 \n",
            "Correct predictions during epoch: 40648 \n",
            "Training accuracy: 79.39\n",
            "\n",
            "Validation loss: 0.4944 \n",
            "Validation accuracy: 62.20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 69%|██████▉   | 11/16 [26:35<12:06, 145.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.0660 \n",
            "Correct predictions during epoch: 42788 \n",
            "Training accuracy: 83.57\n",
            "\n",
            "Validation loss: 0.5147 \n",
            "Validation accuracy: 62.86\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 12/16 [29:00<09:40, 145.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.0566 \n",
            "Correct predictions during epoch: 43705 \n",
            "Training accuracy: 85.36\n",
            "\n",
            "Validation loss: 0.5484 \n",
            "Validation accuracy: 63.15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81%|████████▏ | 13/16 [31:25<07:15, 145.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.0518 \n",
            "Correct predictions during epoch: 44327 \n",
            "Training accuracy: 86.58\n",
            "\n",
            "Validation loss: 0.5706 \n",
            "Validation accuracy: 63.42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 14/16 [33:50<04:50, 145.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.0472 \n",
            "Correct predictions during epoch: 44843 \n",
            "Training accuracy: 87.59\n",
            "\n",
            "Validation loss: 0.5275 \n",
            "Validation accuracy: 63.27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 15/16 [36:16<02:25, 145.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.0438 \n",
            "Correct predictions during epoch: 45379 \n",
            "Training accuracy: 88.63\n",
            "\n",
            "Validation loss: 0.5293 \n",
            "Validation accuracy: 62.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [38:41<00:00, 145.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.0366 \n",
            "Correct predictions during epoch: 46312 \n",
            "Training accuracy: 90.45\n",
            "\n",
            "Validation loss: 0.5660 \n",
            "Validation accuracy: 63.47\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving model and loading model parameters"
      ],
      "metadata": {
        "id": "GgcOiBFJXGS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), './final_model_attention.pt')"
      ],
      "metadata": {
        "id": "0SmMaYvaXMCg"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Seq2Seq(encoder_net, decoder_net, cell_type_encoder, cell_type_decoder, bidirectionality).to(device)\n",
        "model.load_state_dict(torch.load('./final_model_attention.pt'))"
      ],
      "metadata": {
        "id": "3WGM3wZ0XRmM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f4b85c-2151-4cad-e707-ec39dd8841d8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = len(test_dataset)"
      ],
      "metadata": {
        "id": "BVt47j5vXU16"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size, shuffle=False, num_workers = 2)"
      ],
      "metadata": {
        "id": "O1Mwsl4_XWfS"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy, attention_scores = accuracy(test_loader)\n",
        "print('Test loss: %.4f'% test_loss, '\\nTest accuracy: %.2f'% test_accuracy)"
      ],
      "metadata": {
        "id": "hNje6bZMXZYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0e71da7-2cc3-4f05-b39a-61de268bcaa0"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.7663 \n",
            "Test accuracy: 52.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(attention_scores.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uuk75aMgojWA",
        "outputId": "b3c87b88-cf73-4499-ae4f-c26e2e7a31ec"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 4096, 1])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "uixetIxwm_qE",
        "JEItPR0Hp904",
        "L6B8VQgLq0Na",
        "zNOalLjTgryp",
        "LP7BX91WRUeS"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}