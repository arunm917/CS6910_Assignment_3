{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arunm917/CS6910_Assignment_3/blob/main/CS6910_Assignment_3_PartA_V5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uixetIxwm_qE"
      },
      "source": [
        "# Downloading necessary packages and files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkx75o8qZtGB"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import gdown\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jkkc0LcC4whY"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHm92AkybYZb",
        "outputId": "7ff5f80d-dd46-4482-fe42-c6515098d807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1pdJVD8P71fpqGRnvFfOp_6TbVft9NlnH\n",
            "To: /content/tam_train\n",
            "100%|██████████| 2.69M/2.69M [00:00<00:00, 91.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# downloading file from gdrive\n",
        "output = 'tam_train'\n",
        "file_id = '1pdJVD8P71fpqGRnvFfOp_6TbVft9NlnH' # Google drive ID\n",
        "#Download the file\n",
        "gdown.download('https://drive.google.com/uc?id=' + file_id, output, quiet=False)\n",
        "print('DONE.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AVeCbeNnQuw",
        "outputId": "51035f0c-6ed7-43f1-979f-3eefe6e240be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1pdp6ojHltRRNLXsmoQbGRc2Qn8X1EUJV\n",
            "To: /content/tam_valid\n",
            "100%|██████████| 164k/164k [00:00<00:00, 13.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# downloading file from gdrive\n",
        "output = 'tam_valid'\n",
        "file_id = '1pdp6ojHltRRNLXsmoQbGRc2Qn8X1EUJV' # Google drive ID\n",
        "#Download the file\n",
        "gdown.download('https://drive.google.com/uc?id=' + file_id, output, quiet=False)\n",
        "print('DONE.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgydhFEdnUZI",
        "outputId": "be15a700-010c-438d-f776-8e07e5747ef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1pdaTq-g2ZKhRKv6fRrSbEsJkOH5gdrEQ\n",
            "To: /content/tam_test\n",
            "100%|██████████| 157k/157k [00:00<00:00, 128MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# downloading file from gdrive\n",
        "output = 'tam_test'\n",
        "file_id = '1pdaTq-g2ZKhRKv6fRrSbEsJkOH5gdrEQ' # Google drive ID\n",
        "#Download the file\n",
        "gdown.download('https://drive.google.com/uc?id=' + file_id, output, quiet=False)\n",
        "print('DONE.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEItPR0Hp904"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKNN0xMhaI4L"
      },
      "outputs": [],
      "source": [
        "train_data_df = pd.read_csv('tam_train')\n",
        "valid_data_df = pd.read_csv('tam_valid')\n",
        "test_data_df = pd.read_csv('tam_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiKR6-yZ4pKX"
      },
      "outputs": [],
      "source": [
        "train_data_df.columns = ['English','Tamil']\n",
        "# valid_data_df.columns = ['English','Tamil']\n",
        "# test_data_df.columns = ['English','Tamil']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6B8VQgLq0Na"
      },
      "source": [
        "# Creating vocabulary and padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRuAnljKTPk9"
      },
      "outputs": [],
      "source": [
        "# Creating vocabulary\n",
        "\n",
        "char_list_eng = []\n",
        "for i in range(len(train_data_df['English'])):\n",
        "  char = [*train_data_df.loc[i, 'English']]\n",
        "  char_list_eng.extend(char)\n",
        "\n",
        "char_list_tam = []\n",
        "for i in range(len(train_data_df['Tamil'])):\n",
        "  char = [*train_data_df.loc[i, 'Tamil']]\n",
        "  char_list_tam.extend(char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB3ZXOTcZD5s"
      },
      "outputs": [],
      "source": [
        "# Indexing\n",
        "\n",
        "SOS_token = '<SOS>'\n",
        "EOS_token = '<EOS>'\n",
        "PAD_token = '<PAD>'\n",
        "UNK_token = '<UNK>'\n",
        "\n",
        "vocabulary_eng = list(set(char_list_eng))\n",
        "vocabulary_eng = [PAD_token] + [UNK_token] + [SOS_token] + [EOS_token] + vocabulary_eng \n",
        "\n",
        "vocabulary_tam = list(set(char_list_tam))\n",
        "vocabulary_tam = [PAD_token] + [UNK_token] + [SOS_token] + [EOS_token] + vocabulary_tam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8vdEmMsbw6U"
      },
      "outputs": [],
      "source": [
        "char_index_eng = {value: index for index, value in enumerate(vocabulary_eng)}\n",
        "char_index_tam = {value: index for index, value in enumerate(vocabulary_tam)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4N10f0eMJ7N",
        "outputId": "7f5aec29-36f3-4a62-9212-26a3701eb5a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '<PAD>', 1: '<UNK>', 2: '<SOS>', 3: '<EOS>', 4: 't', 5: 'c', 6: 'u', 7: 's', 8: 'f', 9: 'k', 10: 'x', 11: 'a', 12: 'b', 13: 'o', 14: 'i', 15: 'p', 16: 'l', 17: 'n', 18: 'd', 19: 'j', 20: 'r', 21: 'v', 22: 'e', 23: 'q', 24: 'm', 25: 'g', 26: 'y', 27: 'w', 28: 'z', 29: 'h'}\n",
            "{0: '<PAD>', 1: '<UNK>', 2: '<SOS>', 3: '<EOS>', 4: 'க', 5: 'எ', 6: 'உ', 7: 'த', 8: 'ள', 9: 'ை', 10: 'ண', 11: 'ட', 12: 'ஒ', 13: 'ெ', 14: 'ே', 15: 'ஊ', 16: 'ந', 17: 'இ', 18: 'ஹ', 19: 'ஸ', 20: 'ர', 21: 'ஓ', 22: 'ற', 23: 'ஃ', 24: 'ா', 25: 'ு', 26: 'ஐ', 27: 'ல', 28: 'வ', 29: 'ஆ', 30: 'ீ', 31: 'ஜ', 32: '்', 33: 'ழ', 34: 'ோ', 35: 'ய', 36: 'ி', 37: 'ொ', 38: 'ச', 39: 'ௌ', 40: 'ப', 41: 'ம', 42: 'ஈ', 43: 'அ', 44: 'ஏ', 45: 'ன', 46: 'ஞ', 47: 'ங', 48: 'ூ', 49: 'ஷ'}\n"
          ]
        }
      ],
      "source": [
        "idx2char_eng = {value: key for key, value in char_index_eng.items()}\n",
        "idx2char_tam = {value: key for key, value in char_index_tam.items()}\n",
        "print(idx2char_eng)\n",
        "print(idx2char_tam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWrNfanAtOvi"
      },
      "outputs": [],
      "source": [
        "# Defining the tokenizer\n",
        "def tokenize_eng(word):\n",
        "    chars = [*word]\n",
        "    tokens_eng = [char_index_eng[char] if char in char_index_eng else 0 for char in chars]\n",
        "    \n",
        "    return tokens_eng\n",
        "\n",
        "def tokenize_tam(word):\n",
        "    chars = [*word]\n",
        "    tokens_tam = [char_index_tam[char] if char in char_index_tam else 0 for char in chars]\n",
        "    \n",
        "    return tokens_tam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDoAvRMB050I"
      },
      "outputs": [],
      "source": [
        "# Define the training pairs\n",
        "training_pairs = train_data_df.values.tolist()\n",
        "val_pairs = valid_data_df.values.tolist()\n",
        "test_pairs = test_data_df.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnKBJPS8BVdM"
      },
      "outputs": [],
      "source": [
        "eng_words = [tokenize_eng(pair[0]) for pair in training_pairs]\n",
        "tam_words = [tokenize_tam(pair[1]) for pair in training_pairs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFD9A6kACNa9"
      },
      "outputs": [],
      "source": [
        "# Determining max length english\n",
        "\n",
        "lengths_eng = []\n",
        "# max_length_eng = max([len(words) for words in eng_words])\n",
        "for word in eng_words:\n",
        "    word_length = len(word)\n",
        "    lengths_eng.append(word_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXirSnvwMRc0"
      },
      "outputs": [],
      "source": [
        "# Determining max length tamil\n",
        "max_length_tam = max([len(words) for words in tam_words])\n",
        "\n",
        "# Determining max length english and tamil\n",
        "max_length = max([len(words) for words in eng_words + tam_words])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jJPAiIbL_cE"
      },
      "outputs": [],
      "source": [
        "def padding(word_pairs):\n",
        "  ''' Function to pad the input and target sequences. Padding is done to ensure that\n",
        "      all the training, validation and test samples are of equal size.'''\n",
        "  \n",
        "  eng_words = [tokenize_eng(pair[0]) for pair in word_pairs]\n",
        "  tam_words = [tokenize_tam(pair[1]) for pair in word_pairs]\n",
        "\n",
        "  \n",
        "  padded_input_sequences = [torch.tensor([char_index_eng['<SOS>']] + eng_words + [char_index_eng['<EOS>']] + [(char_index_eng['<PAD>'])]*(max_length - len(eng_words))) for eng_words in eng_words]\n",
        "  padded_target_sequences = [torch.tensor([char_index_eng['<SOS>']] + tam_words + [char_index_tam['<EOS>']] + [(char_index_tam['<PAD>'])]*(max_length - len(tam_words))) for tam_words in tam_words]\n",
        "  tensor = torch.tensor([char_index_eng['<PAD>']]*(max_length+2))\n",
        "  padded_input_sequences.append(tensor)\n",
        "  padded_target_sequences.append(tensor)\n",
        "  padded_input_sequences = torch.stack(padded_input_sequences)\n",
        "  padded_target_sequences = torch.stack(padded_target_sequences)\n",
        "  \n",
        "  return(padded_input_sequences,padded_target_sequences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mLjEjNevW_4"
      },
      "outputs": [],
      "source": [
        "# Creating datasets\n",
        "training_input_sequences, training_target_sequences = padding(training_pairs)\n",
        "train_dataset = torch.utils.data.TensorDataset(training_input_sequences, training_target_sequences)\n",
        "\n",
        "val_input_sequences, val_target_sequences = padding(val_pairs)\n",
        "val_dataset = torch.utils.data.TensorDataset(val_input_sequences, val_target_sequences)\n",
        "\n",
        "test_input_sequences, test_target_sequences = padding(test_pairs)\n",
        "test_dataset = torch.utils.data.TensorDataset(test_input_sequences, test_target_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNOalLjTgryp"
      },
      "source": [
        "# Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sE5HL5KeBOkS"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, bidirectionality, cell_type_encoder):\n",
        "        super(Encoder, self).__init__()\n",
        "        ''' The encoder encodes the input characters and converts it into a hidden representation'''\n",
        "\n",
        "        self.bidirectionality = bidirectionality # bidirectionality adds another layer to the RNN which reads the characters in the reverse direction\n",
        "\n",
        "        if self.bidirectionality == 'YES':\n",
        "          bidirectional = True\n",
        "          self.directions = 2\n",
        "        else:\n",
        "          bidirectional = False\n",
        "          self.directions = 1\n",
        "\n",
        "        self.cell_type_encoder = cell_type_encoder\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "\n",
        "        # The lines below implement the different types of RNN units used\n",
        "        if self.cell_type_encoder == 'RNN':\n",
        "          self.rnn = nn.RNN(embedding_size, hidden_size, num_layers,bidirectional = bidirectional, dropout=p)\n",
        "        if self.cell_type_encoder == 'LSTM':\n",
        "          self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers,bidirectional = bidirectional, dropout=p)\n",
        "        if self.cell_type_encoder == 'GRU':\n",
        "          self.rnn = nn.GRU(embedding_size, hidden_size, num_layers,bidirectional = bidirectional, dropout=p)\n",
        "\n",
        "        self.fc_hidden = nn.Linear(hidden_size*self.directions, hidden_size)\n",
        "        self.fc_cell = nn.Linear(hidden_size*self.directions, hidden_size)\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is the input sequence of shape: (seq_length, N) where N is batch size\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        # embeddings created using nn.Embedding of shape: (seq_length, N, embedding_size)\n",
        "\n",
        "        if self.cell_type_encoder == 'LSTM':\n",
        "          outputs, (hidden, cell) = self.rnn(embedding)\n",
        "        else:\n",
        "          outputs, hidden = self.rnn(embedding)\n",
        "\n",
        "        '''After recieving the hidden states from the bidriectional RNN, the number of layers get\n",
        "           converted to num_layers*2. Hence the forward and backward directions have to be concatenated\n",
        "           and resized to the correct dimensions. The fc_hidden and fc_cell layers are used for resizing\n",
        "           the dimensions.\n",
        "           The steps below perform the above mentioned operations for the case of LSTM, GRU and RNN.\n",
        "           Bidirectionality can be switched off if not required'''\n",
        "\n",
        "        if self.cell_type_encoder == 'LSTM':\n",
        "          # This implementation is for LSTM\n",
        "          if self.bidirectionality == 'YES':\n",
        "            row = 1\n",
        "            hidden_list = []\n",
        "            cell_list = []\n",
        "            ''' The lines below implement the contamination operation'''\n",
        "            for i in range(hidden.shape[0]//2):\n",
        "              hidden_concatenated = self.fc_hidden(torch.cat((hidden[row-1:row], hidden[row:row+1]), dim=2)) \n",
        "              cell_concatenated = self.fc_cell(torch.cat((cell[row-1:row], cell[row:row+1]), dim=2))\n",
        "              hidden_list.append(hidden_concatenated)\n",
        "              cell_list.append(cell_concatenated)\n",
        "              row += 2\n",
        "\n",
        "            hidden_tensor = torch.stack(hidden_list)\n",
        "            cell_tensor = torch.stack(cell_list)\n",
        "            hidden_squeezed = hidden_tensor.squeeze()\n",
        "            cell_squeezed = cell_tensor.squeeze()\n",
        "            # If bidirectionality is switched off there is not concatenation\n",
        "          else:\n",
        "            hidden_squeezed = hidden \n",
        "            cell_squeezed = cell\n",
        "        else:\n",
        "          # The following implementation is for RNN and GRU\n",
        "          if self.bidirectionality == 'YES':\n",
        "            row = 1\n",
        "            hidden_list = []\n",
        "            for i in range(hidden.shape[0]//2):\n",
        "              # print('i',i)\n",
        "              hidden_concatenated = self.fc_hidden(torch.cat((hidden[row-1:row], hidden[row:row+1]), dim=2))\n",
        "              # print('hidden in for loop', hidden_bid.shape)\n",
        "              hidden_list.append(hidden_concatenated)\n",
        "              row += 2\n",
        "\n",
        "            hidden_tensor = torch.stack(hidden_list)\n",
        "            hidden_squeezed = hidden_tensor.squeeze()\n",
        "          else:\n",
        "            hidden_squeezed = hidden\n",
        "\n",
        "        if self.cell_type_encoder == 'LSTM':\n",
        "          return hidden_squeezed, cell_squeezed\n",
        "        else:\n",
        "          return hidden_squeezed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYYKaTSEBWll"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  ''' The decoder decodes the input from the encoder and produces an output at each timestep '''\n",
        "    def __init__(\n",
        "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p, cell_type_decoder):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type_decoder = cell_type_decoder\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "\n",
        "        # The lines below implement the different types of RNN units used\n",
        "        if self.cell_type_decoder == 'RNN':\n",
        "          self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout=p)\n",
        "        if self.cell_type_decoder == 'LSTM':\n",
        "          self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
        "        if self.cell_type_decoder == 'GRU':\n",
        "          self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, dropout=p)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell = None):\n",
        "        \n",
        "        x = x.unsqueeze(0)\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        if self.cell_type_decoder == 'LSTM':\n",
        "          outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "        else:\n",
        "          outputs, hidden = self.rnn(embedding, hidden)\n",
        "        predictions = self.fc(outputs)\n",
        "\n",
        "        # Reshaping predictions to send it to loss function we want it to be (N, length_target_vocabulary)\n",
        "        predictions = predictions.squeeze(0)\n",
        "        if self.cell_type_decoder == 'LSTM':\n",
        "          return predictions, hidden, cell\n",
        "        else:\n",
        "          return predictions, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmX2n-uzBlIM"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  ''' This is the model. This function recieves the input and sends it to the encoder and receives the output \n",
        "      and the hidden states from the encoder and sends it to the decoder for decoding'''\n",
        "    def __init__(self, encoder, decoder, cell_type_encoder, cell_type_decoder, bidirectionality):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.cell_type_encoder = cell_type_encoder\n",
        "        self.cell_type_decoder = cell_type_decoder\n",
        "        if bidirectionality == 'YES':\n",
        "          bidirectional = True\n",
        "          self.directions = 2\n",
        "        else:\n",
        "          bidirectional = False\n",
        "          self.directions = 1\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "        batch_size = source.shape[1]\n",
        "        target_len = target.shape[0]\n",
        "        target_vocab_size = len(vocabulary_tam)\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "        if self.cell_type_encoder != 'LSTM':\n",
        "          cell = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
        "\n",
        "        if self.cell_type_encoder == 'LSTM':\n",
        "          hidden, cell = self.encoder(source)\n",
        "        else:\n",
        "          hidden = self.encoder(source)\n",
        "\n",
        "        # first input to the Decoder will be <SOS> token\n",
        "        x = target[0]\n",
        "        predicted_sequences = torch.zeros([32, batch_size]).to(device)\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            # Use previous hidden, cell as context from encoder at start\n",
        "            # output, hidden = self.decoder(x, hidden)\n",
        "            if self.cell_type_decoder == 'LSTM':\n",
        "              output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "            else:\n",
        "              output, hidden = self.decoder(x, hidden)\n",
        "\n",
        "            # Store next output prediction\n",
        "            outputs[t] = output\n",
        "\n",
        "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
        "            predicted_token = output.argmax(1)\n",
        "            x = target[t] if random.random() < teacher_force_ratio else predicted_token # Teacher forcing is used to \n",
        "                                                                                        #ensure the model is not making too many mistakes during initial training\n",
        "            predicted_sequences[t] = predicted_token.squeeze()\n",
        "\n",
        "        predicted_sequences_copy = predicted_sequences[1:].t()\n",
        "        target_copy = target[1:].t()\n",
        "        correct_predictions_batch = correct_sequences_count(predicted_sequences_copy, target_copy)\n",
        "        return outputs, correct_predictions_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARtny7rgRiGf"
      },
      "outputs": [],
      "source": [
        "def correct_sequences_count(predicted_sequences, target_sequences):\n",
        "''' This function is used to aggregate the predicted vectors and check if the predictions match the\n",
        "    target words. The function returns the number of correct predictions per batch'''\n",
        "    correct_predictions_batch = 0\n",
        "    words = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        target_word_list = []\n",
        "        target_word_length = 0\n",
        "        predicted_word_length = 0\n",
        "        flag_target = 1\n",
        "        flag_predicted = 1\n",
        "\n",
        "        for element in target_sequences[i]:\n",
        "            idx = element.item()\n",
        "            target_char =  idx2char_tam[idx]\n",
        "            target_word_list.append(target_char)\n",
        "            if flag_target == 1:\n",
        "              target_word_length += 1\n",
        "              if idx == char_index_tam['<EOS>']:\n",
        "                flag_target = 0\n",
        "                break\n",
        "\n",
        "        target_word_length = target_word_length - 1\n",
        "        target_word = ''.join(target_word_list[:-1])\n",
        "    \n",
        "        predicted_word_list = []\n",
        "        for element in predicted_sequences[i]:\n",
        "            idx = element.item()\n",
        "            predicted_char =  idx2char_tam[idx]\n",
        "            predicted_word_list.append(predicted_char)\n",
        "            if flag_predicted == 1:\n",
        "              predicted_word_length += 1\n",
        "              if idx == char_index_tam['<EOS>']:\n",
        "                flag_predicted = 0\n",
        "                break\n",
        "        \n",
        "        predicted_word_length = predicted_word_length - 1\n",
        "        predicted_word = ''.join(predicted_word_list[:-1])\n",
        "        words.append([target_word, predicted_word])\n",
        "        \n",
        "        if target_word_length == predicted_word_length:\n",
        "          if all(x == y for x, y in zip(target_word_list, predicted_word_list)):\n",
        "              correct_predictions_batch += 1\n",
        "    '''Use the below lines only when the predictions need to be written to a text file'''\n",
        "    # with open('predictions_vanilla.txt', 'w') as file:\n",
        "    #     # Write each predicted word to the file\n",
        "    #     for word in words[:-1]:\n",
        "    #         line = str(word) + '\\n'\n",
        "    #         file.write(line)\n",
        "    ''' End of code for writing into text file '''\n",
        "    return correct_predictions_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M26OZJMzZYrN"
      },
      "outputs": [],
      "source": [
        "def accuracy(dataloader):\n",
        "  ''' This function predicts the accuracy by getting the correct words from correct_sequences_count function'''\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    total_loss = 0\n",
        "    correct_predictions_total = 0\n",
        "    correct_predictions_batch = 0\n",
        "\n",
        "    for batch_idx, (input_seq, target_seq) in enumerate(dataloader):\n",
        "        batch_idx += 1\n",
        "        # Get input and targets and get to cuda\n",
        "        inp_data = input_seq.t().to(device)\n",
        "        target = target_seq.t().to(device)\n",
        "\n",
        "        # Forward prop\n",
        "        output, correct_predictions_batch = model(inp_data, target)\n",
        "        output = output[1:].reshape(-1, output.shape[2])\n",
        "        target = target[1:].reshape(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, target)\n",
        "        total_loss += loss.item()\n",
        "        correct_predictions_total += correct_predictions_batch\n",
        "    eval_loss = total_loss/batch_idx\n",
        "    accuracy = (correct_predictions_total/((batch_idx*batch_size) - 1))*100\n",
        "    model.train()\n",
        "  return eval_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP7BX91WRUeS"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VshMHcHWyoa5"
      },
      "outputs": [],
      "source": [
        "input_size_encoder = len(vocabulary_eng)\n",
        "input_size_decoder = len(vocabulary_tam)\n",
        "output_size = len(vocabulary_tam)\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 18\n",
        "learning_rate = 0.001\n",
        "batch_size = 256\n",
        "embedding_size = 200\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "enc_dropout = 0.5\n",
        "dec_dropout = 0.5\n",
        "bidirectionality = 'YES'\n",
        "cell_type_encoder = 'LSTM'\n",
        "cell_type_decoder = 'LSTM'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkv626hTF5L7"
      },
      "outputs": [],
      "source": [
        "encoder_net = Encoder(input_size_encoder, embedding_size, hidden_size, num_layers, enc_dropout, bidirectionality, cell_type_encoder).to(device)\n",
        "decoder_net = Decoder(input_size_decoder,embedding_size, hidden_size, output_size, num_layers, dec_dropout, cell_type_decoder).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79LI_XBFG8ey"
      },
      "outputs": [],
      "source": [
        "model = Seq2Seq(encoder_net, decoder_net, cell_type_encoder, cell_type_decoder, bidirectionality).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "pad_idx = char_index_eng['<PAD>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "scheduler = StepLR(optimizer, step_size = 5, gamma = 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4In6N1XzgpJd"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JaFVJCnIY3V"
      },
      "outputs": [],
      "source": [
        "# Creating Dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True, num_workers = 2)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size, shuffle=True, num_workers = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNT0YIUwdybM",
        "outputId": "fa361518-8862-4ef1-9d2c-265713520aa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 1/18 [03:05<52:32, 185.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 1.6366 \n",
            "Correct predictions per epoch: 2160 \n",
            "Training accuracy: 4.22\n",
            "\n",
            "Validation loss: 0.6730 \n",
            "Validation accuracy: 33.58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 2/18 [06:09<49:18, 184.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.4256 \n",
            "Correct predictions per epoch: 21218 \n",
            "Training accuracy: 41.44\n",
            "\n",
            "Validation loss: 0.4701 \n",
            "Validation accuracy: 52.94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 3/18 [09:14<46:10, 184.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.2189 \n",
            "Correct predictions per epoch: 31972 \n",
            "Training accuracy: 62.45\n",
            "\n",
            "Validation loss: 0.4461 \n",
            "Validation accuracy: 57.58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 4/18 [12:18<43:01, 184.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.1579 \n",
            "Correct predictions per epoch: 35940 \n",
            "Training accuracy: 70.20\n",
            "\n",
            "Validation loss: 0.4490 \n",
            "Validation accuracy: 58.78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 5/18 [15:22<39:54, 184.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.1218 \n",
            "Correct predictions per epoch: 38723 \n",
            "Training accuracy: 75.63\n",
            "\n",
            "Validation loss: 0.4450 \n",
            "Validation accuracy: 60.12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 6/18 [18:26<36:49, 184.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.0699 \n",
            "Correct predictions per epoch: 43493 \n",
            "Training accuracy: 84.95\n",
            "\n",
            "Validation loss: 0.4783 \n",
            "Validation accuracy: 61.90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39%|███▉      | 7/18 [21:31<33:48, 184.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.0456 \n",
            "Correct predictions per epoch: 45832 \n",
            "Training accuracy: 89.52\n",
            "\n",
            "Validation loss: 0.5009 \n",
            "Validation accuracy: 61.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 8/18 [24:34<30:41, 184.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.0350 \n",
            "Correct predictions per epoch: 47305 \n",
            "Training accuracy: 92.39\n",
            "\n",
            "Validation loss: 0.4926 \n",
            "Validation accuracy: 62.30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 9/18 [27:38<27:37, 184.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.0276 \n",
            "Correct predictions per epoch: 48317 \n",
            "Training accuracy: 94.37\n",
            "\n",
            "Validation loss: 0.5230 \n",
            "Validation accuracy: 62.91\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 10/18 [30:42<24:32, 184.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.0246 \n",
            "Correct predictions per epoch: 48783 \n",
            "Training accuracy: 95.28\n",
            "\n",
            "Validation loss: 0.5313 \n",
            "Validation accuracy: 62.32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61%|██████    | 11/18 [33:46<21:28, 184.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.0146 \n",
            "Correct predictions per epoch: 50055 \n",
            "Training accuracy: 97.77\n",
            "\n",
            "Validation loss: 0.5566 \n",
            "Validation accuracy: 62.59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 12/18 [36:50<18:23, 183.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.0086 \n",
            "Correct predictions per epoch: 50690 \n",
            "Training accuracy: 99.01\n",
            "\n",
            "Validation loss: 0.6299 \n",
            "Validation accuracy: 62.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 13/18 [39:54<15:20, 184.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.0066 \n",
            "Correct predictions per epoch: 50856 \n",
            "Training accuracy: 99.33\n",
            "\n",
            "Validation loss: 0.5790 \n",
            "Validation accuracy: 63.49\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 14/18 [42:58<12:15, 183.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.0052 \n",
            "Correct predictions per epoch: 50949 \n",
            "Training accuracy: 99.51\n",
            "\n",
            "Validation loss: 0.5790 \n",
            "Validation accuracy: 63.17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 15/18 [46:02<09:11, 183.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.0047 \n",
            "Correct predictions per epoch: 50986 \n",
            "Training accuracy: 99.58\n",
            "\n",
            "Validation loss: 0.6190 \n",
            "Validation accuracy: 63.35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89%|████████▉ | 16/18 [49:06<06:08, 184.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.0038 \n",
            "Correct predictions per epoch: 51037 \n",
            "Training accuracy: 99.68\n",
            "\n",
            "Validation loss: 0.6501 \n",
            "Validation accuracy: 63.52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 17/18 [52:10<03:03, 183.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.0030 \n",
            "Correct predictions per epoch: 51105 \n",
            "Training accuracy: 99.82\n",
            "\n",
            "Validation loss: 0.6376 \n",
            "Validation accuracy: 63.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18/18 [55:14<00:00, 184.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc loss: 0.0026 \n",
            "Correct predictions per epoch: 51122 \n",
            "Training accuracy: 99.85\n",
            "\n",
            "Validation loss: 0.6420 \n",
            "Validation accuracy: 63.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "\n",
        "    total_loss = 0\n",
        "    correct_predictions_epoch = 0\n",
        "    correct_predictions_batch = 0\n",
        "\n",
        "    for batch_idx, (input_seq, target_seq) in enumerate(train_loader):\n",
        "        batch_idx += 1\n",
        "        # Get input and targets and assign to cuda\n",
        "        inp_data = input_seq.t().to(device)\n",
        "        target = target_seq.t().to(device)\n",
        "\n",
        "        # Forward prop\n",
        "        output, correct_predictions_batch = model(inp_data, target)\n",
        "        output = output[1:].reshape(-1, output.shape[2])\n",
        "        target = target[1:].reshape(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, target)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Back prop\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip to avoid exploding gradient issues\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "        # Gradient descent step\n",
        "        optimizer.step()\n",
        "        correct_predictions_epoch += correct_predictions_batch\n",
        "    scheduler.step()\n",
        "    loss_epoch = total_loss/batch_idx\n",
        "    train_accuracy = (correct_predictions_epoch/((batch_idx*batch_size) - 1))*100\n",
        "    val_loss, val_accuracy = accuracy(val_loader)\n",
        "    print('\\nEpoc loss: %.4f' % loss_epoch, '\\nCorrect predictions per epoch:',correct_predictions_epoch,\n",
        "          '\\nTraining accuracy: %.2f'% train_accuracy)\n",
        "    print('\\nValidation loss: %.4f'% val_loss, '\\nValidation accuracy: %.2f'% val_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Saving and loading model parameters"
      ],
      "metadata": {
        "id": "hifnGxh4_Yvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), './final_model_vanilla.pt')"
      ],
      "metadata": {
        "id": "AvLCPNcx-HCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Seq2Seq(encoder_net, decoder_net, cell_type_encoder, cell_type_decoder, bidirectionality).to(device)\n",
        "model.load_state_dict(torch.load('./final_model_vanilla.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MouYHh1_Ahug",
        "outputId": "f98e84a4-1d16-4e79-c596-3702e2f27b36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = len(test_dataset)"
      ],
      "metadata": {
        "id": "wMPHMUWbL5X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size, shuffle=False, num_workers = 2)"
      ],
      "metadata": {
        "id": "xaYymD_BLyJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = accuracy(test_loader)\n",
        "print('Test loss: %.4f'% test_loss, '\\nTest accuracy: %.2f'% test_accuracy)"
      ],
      "metadata": {
        "id": "d3sDWxjOArav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f04e1fb9-572e-4ee1-84b5-9de70ecd3e4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.1347 \n",
            "Test accuracy: 53.38\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JEItPR0Hp904",
        "L6B8VQgLq0Na"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}